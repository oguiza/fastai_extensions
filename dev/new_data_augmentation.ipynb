{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:36:54.886125Z",
     "start_time": "2019-07-15T14:36:54.880920Z"
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "from fastai.torch_core import *\n",
    "from fastai.callback import *\n",
    "from fastai.train import mixup\n",
    "from fastai.callbacks.mixup import MixUpCallback, MixUpLoss\n",
    "from fastai.basic_train import Learner, LearnerCallback\n",
    "from fastai.vision.image import Image, TfmPixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:36:55.039378Z",
     "start_time": "2019-07-15T14:36:55.016230Z"
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "class RicapLoss(nn.Module):\n",
    "    \"Adapt the loss function `crit` to go with ricap data augmentations.\"\n",
    "\n",
    "    def __init__(self, crit, reduction='mean'):\n",
    "        super().__init__()\n",
    "        if hasattr(crit, 'reduction'):\n",
    "            self.crit = crit\n",
    "            self.old_red = crit.reduction\n",
    "            setattr(self.crit, 'reduction', 'none')\n",
    "        else:\n",
    "            self.crit = partial(crit, reduction='none')\n",
    "            self.old_crit = crit\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        if target.ndim == 2:\n",
    "            c_ = target[:, 1:5]\n",
    "            W_ = target[:, 5:]\n",
    "            loss = [W_[:, k] * self.crit(output, c_[:, k].long()) for k in range(4)]\n",
    "            d = torch.mean(torch.stack(loss))\n",
    "        else: d = self.crit(output, target)\n",
    "        if self.reduction == 'mean': return d.mean()\n",
    "        elif self.reduction == 'sum': return d.sum()\n",
    "        return d\n",
    "\n",
    "    def get_old(self):\n",
    "        if hasattr(self, 'old_crit'): return self.old_crit\n",
    "        elif hasattr(self, 'old_red'):\n",
    "            setattr(self.crit, 'reduction', self.old_red)\n",
    "            return self.crit\n",
    "\n",
    "class RicapCallback(LearnerCallback):\n",
    "    '''Adapted from : \n",
    "    paper: https://arxiv.org/abs/1811.09030\n",
    "    github: https://github.com/4uiiurz1/pytorch-ricap\n",
    "    and mixup in the fastai library.'''\n",
    "    def __init__(self, learn:Learner, beta:float=.3, stack_y:bool=True):\n",
    "        super().__init__(learn)\n",
    "        self.beta,self.stack_y = beta,stack_y\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if self.stack_y: self.learn.loss_func = RicapLoss(self.learn.loss_func)\n",
    "\n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        \"Applies ricap to `last_input` and `last_target` if `train`.\"\n",
    "        if not train or self.beta == 0: return\n",
    "        \n",
    "        # get the image size\n",
    "        I_x, I_y = last_input.size()[2:]\n",
    "        \n",
    "        # draw a boundary position (w, h)\n",
    "        w = int(np.round(I_x * np.random.beta(self.beta, self.beta)))\n",
    "        h = int(np.round(I_y * np.random.beta(self.beta, self.beta)))\n",
    "        w_ = [w, I_x - w, w, I_x - w]\n",
    "        h_ = [h, h, I_y - h, I_y - h]\n",
    "       \n",
    "        # select and crop four images\n",
    "        cropped_images = {}\n",
    "        bs = last_input.size(0)\n",
    "        c_ = torch.zeros((bs, 4)).float().to(last_input.device)\n",
    "        W_ = torch.zeros(4).float().to(last_input.device)\n",
    "        for k in range(4):\n",
    "            idx = torch.randperm(bs).to(last_input.device)\n",
    "            x_k = np.random.randint(0, I_x - w_[k] + 1)\n",
    "            y_k = np.random.randint(0, I_y - h_[k] + 1)\n",
    "            cropped_images[k] = last_input[idx][:, :, x_k:x_k + w_[k], y_k:y_k + h_[k]]\n",
    "            c_[:, k] = last_target[idx].float()\n",
    "            W_[k] = w_[k] * h_[k] / (I_x * I_y)\n",
    "        \n",
    "        # patch cropped images\n",
    "        patched_images = torch.cat(\n",
    "            (torch.cat((cropped_images[0], cropped_images[1]), 2),\n",
    "             torch.cat((cropped_images[2], cropped_images[3]), 2)), 3).to(last_input.device)\n",
    "\n",
    "        # modify last target\n",
    "        if self.stack_y:\n",
    "                new_target = torch.cat((last_target[:,None].float(), c_,\n",
    "                                        W_[None].repeat(last_target.size(0), 1)), dim=1)\n",
    "        else:\n",
    "            new_target = c_ * W_\n",
    "        \n",
    "        return {'last_input': patched_images, 'last_target': new_target}\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()\n",
    "\n",
    "\n",
    "def ricap(learn:Learner, beta:float=.3, stack_y:bool=True) -> Learner:\n",
    "    \"Add ricap https://arxiv.org/pdf/1811.09030.pdf to `learn`.\"\n",
    "    learn.callback_fns.append(partial(RicapCallback, beta=beta, stack_y=stack_y))\n",
    "    return learn\n",
    "\n",
    "setattr(ricap, 'cb_fn', RicapCallback)\n",
    "Learner.ricap = ricap\n",
    "\n",
    "setattr(mixup, 'cb_fn', MixUpCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:36:55.292203Z",
     "start_time": "2019-07-15T14:36:55.274007Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class CutMixCallback(LearnerCallback):\n",
    "    '''Adapted from : \n",
    "    paper: https://arxiv.org/abs/1905.04899\n",
    "    github: https://github.com/clovaai/CutMix-PyTorch\n",
    "    and mixup in the fastai library.'''\n",
    "    \n",
    "    def __init__(self, learn:Learner, alpha:float=1., stack_y:bool=True, true_λ:bool=True):\n",
    "        super().__init__(learn)\n",
    "        self.alpha,self.stack_y,self.true_λ = alpha,stack_y,true_λ\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if self.stack_y: self.learn.loss_func = MixUpLoss(self.learn.loss_func)\n",
    "\n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        \"Applies cutmix to `last_input` and `last_target` if `train`.\"\n",
    "        if not train or self.alpha == 0: return\n",
    "        λ = np.random.beta(self.alpha, self.alpha)\n",
    "        λ = max(λ, 1- λ)\n",
    "        bs = last_target.size(0)\n",
    "        idx = torch.randperm(bs).to(last_input.device)\n",
    "        x1, y1 = last_input[idx], last_target[idx]\n",
    "\n",
    "        #Get new input\n",
    "        last_input_size = last_input.size()\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(last_input_size, λ)\n",
    "        new_input = last_input.clone()\n",
    "        new_input[..., bby1:bby2, bbx1:bbx2] = x1[..., bby1:bby2, bbx1:bbx2]\n",
    "        λ = last_input.new([λ])\n",
    "        λ = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (last_input_size[-1] * last_input_size[-2]))\n",
    "        λ = last_input.new([λ])\n",
    "\n",
    "        # modify last target\n",
    "        if self.stack_y:\n",
    "            new_target = torch.cat([last_target.unsqueeze(1).float(), y1.unsqueeze(1).float(),\n",
    "                                    λ.repeat(last_input_size[0]).unsqueeze(1).float()], 1)\n",
    "        else:\n",
    "            if len(last_target.shape) == 2:\n",
    "                λ = λ.unsqueeze(1).float()\n",
    "            new_target = last_target.float() * λ + y1.float() * (1-λ)\n",
    "        \n",
    "        return {'last_input': new_input, 'last_target': new_target}\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()\n",
    "\n",
    "\n",
    "def rand_bbox(last_input_size, λ):\n",
    "    '''lambd is always between .5 and 1'''\n",
    "\n",
    "    W = last_input_size[-1]\n",
    "    H = last_input_size[-2]\n",
    "    cut_rat = np.sqrt(1. - λ) # 0. - .707\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def cutmix(learn:Learner, alpha:float=1., stack_x:bool=False, stack_y:bool=True, true_λ:bool=True) -> Learner:\n",
    "    \"Add cutmix https://arxiv.org/pdf/1905.04899.pdf to `learn`.\"\n",
    "    learn.callback_fns.append(partial(CutMixCallback, alpha=alpha, stack_y=stack_y, true_λ=true_λ))\n",
    "    return learn\n",
    "\n",
    "setattr(cutmix, 'cb_fn', CutMixCallback)\n",
    "Learner.cutmix = cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "def get_x1_coords(x_size, n_patches, same_size=True):\n",
    "    if same_size:\n",
    "        w = np.linspace(0, x_size[-1], num=n_patches[0] + 1).astype(np.int64)\n",
    "        h = np.linspace(0, x_size[-2], num=n_patches[1] + 1).astype(np.int64)\n",
    "    else:\n",
    "        w, h = [], []\n",
    "        if n_patches[0] > 1:\n",
    "            w = sorted(list(np.random.choice(np.arange(0, x_size[-1]), \n",
    "                                             size= n_patches[0] - 1, replace=False)))\n",
    "        if n_patches[1] > 1:\n",
    "            h = sorted(list(np.random.choice(np.arange(0, x_size[-2]), \n",
    "                                             size= n_patches[1] - 1, replace=False)))\n",
    "        w = [0] + w + [x_size[-1]]\n",
    "        h = [0] + h + [x_size[-2]]\n",
    "    patch = []\n",
    "    for i in range(n_patches[0]):\n",
    "        for j in range(n_patches[1]):\n",
    "            patch.append([h[j], h[j + 1], w[i], w[i + 1]])\n",
    "    return patch\n",
    "\n",
    "\n",
    "def get_x1_rand_coords(x_size, n_patches, w, h, same_size=True):\n",
    "    if not same_size:\n",
    "        a = w * h\n",
    "        p = np.random.uniform() + .5\n",
    "        w = max(1, int(p * w))\n",
    "        h = max(1, int(a / w))\n",
    "    w_x = np.random.randint(0, x_size[-1]) if n_patches[0] != 1 else 0\n",
    "    w_xa = np.clip(w_x - w // 2, 0, x_size[-1])\n",
    "    w_xb = np.clip(w_x + w // 2, 0, x_size[-1])\n",
    "    h_x = np.random.randint(0, x_size[-2]) if n_patches[1] != 1 else 0\n",
    "    h_xa = np.clip(h_x - h // 2, 0, x_size[-2])\n",
    "    h_xb = np.clip(h_x + h // 2, 0, x_size[-2])\n",
    "    return h_xa, h_xb, w_xa, w_xb\n",
    "\n",
    "\n",
    "def get_x2_coords(x_size, bby1, bby2, bbx1, bbx2):\n",
    "    w_ = bbx2 - bbx1\n",
    "    h_ = bby2 - bby1\n",
    "    w_k = np.random.randint(0, x_size[-1] - w_) if w_ != x_size[-1] else 0\n",
    "    h_k = np.random.randint(0, x_size[-2] - h_) if h_ != x_size[-2] else 0\n",
    "    return h_k, h_k + h_, w_k, w_k + w_\n",
    "\n",
    "\n",
    "#export \n",
    "\n",
    "class BlendLoss(nn.Module):\n",
    "    \"Adapt the loss function `crit` to go with blend data augmentations.\"\n",
    "\n",
    "    def __init__(self, crit, reduction='mean'):\n",
    "        super().__init__()\n",
    "        if hasattr(crit, 'reduction'):\n",
    "            self.crit = crit\n",
    "            self.old_red = crit.reduction\n",
    "            setattr(self.crit, 'reduction', 'none')\n",
    "        else:\n",
    "            self.crit = partial(crit, reduction='none')\n",
    "            self.old_crit = crit\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        if target.ndim == 2:# and target.shape[-1] >1:\n",
    "            n_mod_patches = (target.shape[-1] - 1) // 2\n",
    "            c_ = target[:, 1:n_mod_patches + 1]\n",
    "            W_ = target[:, n_mod_patches + 1:]\n",
    "            loss = [W_[:, k] * self.crit(output, c_[:, k].long()) for k in range(n_mod_patches)]\n",
    "            d = torch.mean(torch.stack(loss))\n",
    "        else: d = self.crit(output, target)\n",
    "        if self.reduction == 'mean': return d.mean()\n",
    "        elif self.reduction == 'sum': return d.sum()\n",
    "        return d\n",
    "\n",
    "    def get_old(self):\n",
    "        if hasattr(self, 'old_crit'): return self.old_crit\n",
    "        elif hasattr(self, 'old_red'):\n",
    "            setattr(self.crit, 'reduction', self.old_red)\n",
    "            return self.crit\n",
    "        \n",
    "        \n",
    "class BlendCallback(LearnerCallback):\n",
    "    \"Callback that creates the blend input and target.\"\n",
    "    def __init__(self, learn:Learner, \n",
    "                 size:tuple=(.1, .1), alpha:float=1., fixed_proba:float=0., \n",
    "                 blend_type:str='cut', grid:bool=True,\n",
    "                 same_size:bool=True, same_crop:bool=True, same_image:bool=False):\n",
    "        ''' Modifies one or multiple subregions of an image\n",
    "        Parameters:\n",
    "        size: \n",
    "            int tuple(height pixels, wide pixels), float tuple (height % img, wide % img)\n",
    "            int (pixels) or float (percent_tuple)\n",
    "            None full image\n",
    "        alpha: proba that each patch is modified from np.random.beta(alpha, alpha)\n",
    "        fixed_proba: proba that each individual patch is modified. If >0 overrides alpha.\n",
    "        blend_type: 'zero', 'noise', 'mix', 'cut' or 'rand'(any of the previous)\n",
    "        grid: True - a grid is applied to the image so that patches never overlap (required in 'mix' and 'cut')\n",
    "        same_size: True - all patches will have approx the same size, otherwise random\n",
    "        same_crop: cropping subregion will be the same as input subregion, otherwise different\n",
    "        same_image: False - cropping image will be different from input image, otherwise same\n",
    "        '''\n",
    "        assert blend_type in ['zero', 'noise', 'mix', 'cut', 'random'], \\\n",
    "        print(\"make sure you select one of these blend_types: 'zero', 'noise', 'mix', 'cut', 'random'\")\n",
    "        if not grid and not same_image: \n",
    "            assert blend_type in ['zero', 'noise'],\\\n",
    "            print('either grid or same_image must be set to True when using', blend_type)\n",
    "        super().__init__(learn)\n",
    "        self.size,self.alpha,self.fixed_proba,self.blend_type = size,alpha,fixed_proba,blend_type\n",
    "        self.grid,self.same_size,self.same_crop,self.same_image = grid,same_size,same_crop,same_image\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.same_image: self.learn.loss_func = BlendLoss(self.learn.loss_func)\n",
    "\n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        \"Applies blend to `last_input` and `last_target` if `train`.\"\n",
    "        if not train: return {'last_input': last_input, 'last_target': last_target}\n",
    "        if self.alpha == 0 and self.fixed_proba == 0: \n",
    "            return {'last_input': last_input, 'last_target': last_target}\n",
    "\n",
    "        x_size = last_input.size()\n",
    "        bs = x_size[0] # batch size\n",
    "        i_h, i_w = x_size[2:] # image height, width\n",
    "        if not isinstance(self.size, tuple): self.size = (self.size, self.size)\n",
    "        p_h, p_w = self.size # patch percent height, width\n",
    "        if p_h == 1 or isinstance(p_h, float): h = int(p_h * i_h) # patch height in pixels\n",
    "        else: h = p_h\n",
    "        if p_w == 1 or isinstance(p_w, float): w = int(p_w * i_w) # patch width in pixels\n",
    "        else: w = p_w\n",
    "        if w == 0 or h == 0: return {'last_input': last_input, 'last_target': last_target}\n",
    "        patched_images = last_input.clone()\n",
    "\n",
    "        # patches that will be modified\n",
    "        n_patches = (i_h // h, i_w // w)\n",
    "        patch_len = n_patches[0] * n_patches[1]\n",
    "        patches = get_x1_coords(x_size, n_patches, same_size=self.same_size)\n",
    "        if self.fixed_proba != 0:\n",
    "            lambd = self.fixed_proba\n",
    "        else:\n",
    "            lambd = np.random.beta(self.alpha, self.alpha)\n",
    "            lambd = max(lambd, 1- lambd)\n",
    "        if patch_len == 1: patch_ids = [0]\n",
    "        elif self.fixed_proba != 0:\n",
    "            patch_ids = np.arange(patch_len)[np.random.rand(patch_len) <= lambd]\n",
    "        else:\n",
    "            patch_ids = np.random.choice(np.arange(patch_len), int(patch_len * (1 - lambd)), replace=False)\n",
    "        n_mod_patches = len(patch_ids)\n",
    "        if n_mod_patches == 0: return {'last_input': last_input, 'last_target': last_target}\n",
    "        #mod_patches = [patches[i] for i in patch_ids]\n",
    "        c_ = torch.zeros((bs, n_mod_patches)).float().to(last_input.device) # patch labels\n",
    "        W_ = torch.zeros(n_mod_patches).float().to(last_input.device) # new weights\n",
    "        idx = torch.linspace(0, bs - 1, steps=bs).to(dtype=torch.int64, device=last_input.device)\n",
    "        if self.blend_type.lower() == 'random': _blend = np.random.choice(['zero', 'noise', 'mix', 'cut'])\n",
    "        else: _blend = self.blend_type.lower()\n",
    "        for i,j in enumerate(patch_ids):\n",
    "            #x1 coordinates\n",
    "            if self.grid: bby1, bby2, bbx1, bbx2 = patches[j]\n",
    "            else: bby1, bby2, bbx1, bbx2 = get_x1_rand_coords(x_size, n_patches, w, h, \n",
    "                                                              same_size=self.same_size)\n",
    "            # Blend\n",
    "            if _blend == 'zero': patched_images[..., bby1:bby2, bbx1:bbx2] = 0\n",
    "            if _blend == 'noise': \n",
    "                noise = last_input.new(np.random.rand(bby2 - bby1, bbx2 - bbx1))\n",
    "                patched_images[..., bby1:bby2, bbx1:bbx2] = noise\n",
    "            else: \n",
    "                if not self.same_image: idx = torch.randperm(bs).to(last_input.device)\n",
    "                #x2 coordinates\n",
    "                if self.same_crop: \n",
    "                    x2 = last_input[idx][..., bby1:bby2, bbx1:bbx2]\n",
    "                else:\n",
    "                    ccy1, ccy2, ccx1, ccx2 = get_x2_coords(x_size, bby1, bby2, bbx1, bbx2)\n",
    "                    x2 = last_input[idx][..., ccy1:ccy2, ccx1:ccx2]\n",
    "                if _blend == 'mix':\n",
    "                    x1 = last_input[..., bby1:bby2, bbx1:bbx2]\n",
    "                    if self.size == (1, 1): \n",
    "                        patched_images[..., bby1:bby2, bbx1:bbx2] = x1 * lambd + x2 * (1 - lambd)\n",
    "                    else: patched_images[..., bby1:bby2, bbx1:bbx2] = x1 * .5 + x2 * .5\n",
    "                if _blend == 'cut':\n",
    "                    patched_images[..., bby1:bby2, bbx1:bbx2] = x2\n",
    "            W_[i] = (bby2 - bby1) * (bbx2 - bbx1) / (i_w * i_h)\n",
    "            c_[:, i] = last_target[idx].float()\n",
    "        # modify last target\n",
    "        if not self.same_image and n_mod_patches > 0:\n",
    "            new_target = torch.cat((last_target[:,None].float(), c_, W_[None].repeat(bs, 1)), dim=1)\n",
    "        else: new_target = last_target\n",
    "        return {'last_input': patched_images, 'last_target': new_target}\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        if not self.same_image: self.learn.loss_func = self.learn.loss_func.get_old()\n",
    "\n",
    "\n",
    "def blend(learn:Learner, size:tuple=(.1, .1), alpha:float=1., fixed_proba:float=0., \n",
    "                 blend_type:str='cut', grid:bool=True,\n",
    "                 same_size:bool=True, same_crop:bool=True, same_image:bool=False) -> Learner:\n",
    "    learn.callback_fns.append(partial(BlendCallback, size=size, alpha=alpha, \n",
    "                                      fixed_proba=fixed_proba, blend_type=blend_type, grid=grid,\n",
    "                                      same_size=same_size, same_crop=same_crop, same_image=same_image))\n",
    "    return learn\n",
    "\n",
    "setattr(blend, 'cb_fn', BlendCallback)\n",
    "Learner.blend = blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "import math\n",
    "\n",
    "class TfmScheduler(LearnerCallback):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 learn: Learner,\n",
    "                 tfm_fn: Callable,\n",
    "                 sch_param: Union[str, StrList],\n",
    "                 sch_val: Union[StartOptEnd, List],\n",
    "                 sch_iter: Optional[StartOptEnd] = None,\n",
    "                 sch_func: Optional[AnnealFunc] = None,\n",
    "                 plot: bool = False,\n",
    "                 test: bool = False,\n",
    "                 **kwargs: Any):\n",
    "\n",
    "        super().__init__(learn)\n",
    "        self.learn = learn\n",
    "        self.batches = math.ceil(len(learn.data.train_ds)/learn.data.train_dl.batch_size)\n",
    "        sch_param = listify(sch_param)\n",
    "        if isinstance(sch_param, (float, int)): sch_val = (0, sch_val)\n",
    "        sch_val = tuplify(sch_val)\n",
    "        if len(sch_param) != len(sch_val): sch_val = sch_val * len(sch_param)\n",
    "        assert len(sch_param) == len(sch_val)\n",
    "        if sch_iter is None: sch_iter = (0., 1.)\n",
    "        sch_iter = tuplify(sch_iter)\n",
    "        if len(sch_param) != len(sch_iter): sch_iter = sch_iter * len(sch_param)\n",
    "        assert len(sch_param) == len(sch_iter)\n",
    "        self.tfm_fn,self.sch_param,self.sch_val,self.sch_iter,self.test = tfm_fn,sch_param,sch_val,sch_iter,test\n",
    "        if sch_func is None: sch_func = annealing_linear\n",
    "        sch_func = listify(sch_func)\n",
    "        if len(sch_param) != len(sch_func): sch_func = sch_func * len(sch_param)\n",
    "        assert len(sch_param) == len(sch_func)\n",
    "        self.sch_func = sch_func\n",
    "        self.plot = plot\n",
    "        if not isinstance(self.tfm_fn, functools.partial): self.tfm_fn = partial(self.tfm_fn)\n",
    "        self.fn = get_fn(self.tfm_fn)\n",
    "        if hasattr(self.fn, 'cb_fn'): self.fn = self.fn.cb_fn\n",
    "        if hasattr(self.fn, 'on_batch_begin'): self.cb = True\n",
    "        else: self.cb = False\n",
    "        \n",
    "        \n",
    "    def on_train_begin(self, n_epochs: int, epoch: int, **kwargs: Any):\n",
    "        if self.cb: self.fn(self.learn).on_train_begin()\n",
    "        total_iters = n_epochs * self.batches\n",
    "        self.scheduler = [None] * len(self.sch_param)\n",
    "        for i in range(len(self.sch_param)):\n",
    "            p = self.sch_param[i]\n",
    "            v = self.sch_val[i]\n",
    "            iters = self.sch_iter[i]\n",
    "            func = self.sch_func[i]\n",
    "            self.scheduler[i] = MyScheduler(total_iters, v, sch_iter=iters, sch_func=func)\n",
    "            s = self.scheduler[i]\n",
    "            a = s.start_val\n",
    "            a_ = []\n",
    "            first_iter = -1\n",
    "            last_iter = 1\n",
    "            for i in range(total_iters): \n",
    "                a = s.step()\n",
    "                if i > 0 and first_iter == -1 and a != a_[-1]: first_iter = (i - 1) / total_iters\n",
    "                elif first_iter != -1 and last_iter == 1 and a == a_[-1]: last_iter = i / total_iters\n",
    "                a_.append(a)\n",
    "            s.restart()\n",
    "            text = '{} between {} and {} in iters {:.2f} to {:.2f}'.format(\n",
    "                p, round(min(a_), 5), round(max(a_), 5), first_iter, last_iter)\n",
    "            print('\\n',text)\n",
    "            if self.plot:\n",
    "                plt.plot(a_)\n",
    "                plt.title(text)\n",
    "                plt.show()\n",
    "                \n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        if self.test: return {'stop_epoch': True, 'stop_training': True, 'skip_validate': True}\n",
    "        if train:\n",
    "            for i, (p, v) in enumerate(zip(self.sch_param, self.sch_val)):\n",
    "                new_v = self.scheduler[i].step()\n",
    "                self.tfm_fn.keywords[p] = new_v\n",
    "            kw = self.tfm_fn.keywords\n",
    "            if self.cb: \n",
    "                return self.fn(self.learn, **kw).on_batch_begin(\n",
    "                    last_input=last_input,last_target=last_target,train=train)\n",
    "            else:\n",
    "                new_input = self.fn(last_input, **kw)\n",
    "                return {'last_input': new_input, 'last_target': last_target}\n",
    "        else: return\n",
    "        \n",
    "    def on_train_end(self, **kwargs):\n",
    "        if self.cb: self.fn(self.learn).on_train_end()\n",
    "    \n",
    "    \n",
    "class MyScheduler():\n",
    "    \"Used to \\\"step\\\" from start,end (`vals`) over `n_iter` iterations on a schedule defined by `func`\"\n",
    "    def __init__(self, total_iters:int, sch_val:StartOptEnd, sch_iter:Optional[StartOptEnd]=None, \n",
    "                 sch_func:Optional[AnnealFunc]=None):\n",
    "        self.total_iters = total_iters\n",
    "        self.start_val,self.end_val = (sch_val[0],sch_val[1]) if is_tuple(sch_val) else (0, sch_val)\n",
    "        if sch_iter is None: self.start_iter,self.end_iter = (0, total_iters)\n",
    "        else: \n",
    "            self.start_iter,self.end_iter = (sch_iter[0],sch_iter[1]) if is_tuple(sch_iter) else (0, sch_iter)\n",
    "            if self.start_iter == 1 or isinstance(self.start_iter, float): \n",
    "                self.start_iter = int(self.start_iter * total_iters)\n",
    "            if self.end_iter == 1 or isinstance(self.end_iter, float): \n",
    "                self.end_iter = int(self.end_iter * total_iters)\n",
    "        self.eff_iters = self.end_iter - self.start_iter\n",
    "        if sch_func is None: self.sch_func = annealing_linear\n",
    "        else: self.sch_func = sch_func\n",
    "        self.n = 0\n",
    "\n",
    "    def restart(self): self.n = 0\n",
    "\n",
    "    def step(self)->Number:\n",
    "        \"Return next value along annealed schedule.\"\n",
    "        self.eff_n = min(max(0, self.n - self.start_iter), self.eff_iters)\n",
    "        out = self.sch_func(self.start_val, self.end_val, min(1, self.eff_n/(self.eff_iters - 1)))\n",
    "        self.n += 1\n",
    "        return out\n",
    "\n",
    "    \n",
    "def cosine_annealing(start:Number, end:Number, pct:float, pct_start=.3, **kwargs)->Number:\n",
    "    \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    if pct <= pct_start: \n",
    "        return annealing_cos(start, end, pct/pct_start)\n",
    "    else: \n",
    "        return annealing_cos(end, start, (pct - pct_start)/(1 - pct_start))\n",
    "    \n",
    "def inv_annealing_poly(start:Number, end:Number, pct:float, degree:Number, **kwargs)->Number:\n",
    "    \"Helper function for `inv_anneal_poly`.\"\n",
    "    return start + (end - start) * (pct)**degree\n",
    "\n",
    "def inv_annealing_cos(start:Number, end:Number, pct:float, **kwargs)->Number:\n",
    "    \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    cos_out = np.cos(np.pi * pct) + 1\n",
    "    return start + (end - start)/2 * cos_out\n",
    "\n",
    "def tuplify(a):\n",
    "    if not isinstance(a, list): a = [a]\n",
    "    for i, x in enumerate(a):\n",
    "        if not isinstance(x, tuple): a[i] = (0, x)\n",
    "    return a\n",
    "\n",
    "def get_fn(a):\n",
    "    while True:\n",
    "        if hasattr(a, 'func'): a = a.func\n",
    "        else: break\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:36:55.325645Z",
     "start_time": "2019-07-15T14:36:55.315896Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def show_multi_img_tfms(learn, rows=3, cols=3, figsize=(8, 8)):\n",
    "    xb, yb = learn.data.one_batch()\n",
    "    tfms = learn.data.train_ds.tfms\n",
    "    for i in range(len(xb)):\n",
    "        xb[i] = Image(xb[i]).apply_tfms(tfms).data\n",
    "    for cb in learn.callback_fns:\n",
    "        try:\n",
    "            cb_fn = partial(cb.func, **cb.keywords)\n",
    "            [Image(cb_fn(learn).on_batch_begin(\n",
    "                        xb, yb, True)['last_input'][0]).show(ax=ax)\n",
    "                for i, ax in enumerate(\n",
    "                    plt.subplots(rows, cols, figsize=figsize)[1].flatten())]\n",
    "            plt.show()\n",
    "            break\n",
    "        except:\n",
    "            plt.close('all')\n",
    "    return learn\n",
    "\n",
    "\n",
    "Learner.show_multi_img_tfms = show_multi_img_tfms\n",
    "\n",
    "\n",
    "def show_single_img_tfms(learn, rows=3, cols=3, figsize=(8, 8)):\n",
    "    img = learn.data.train_ds.x\n",
    "    tfms = learn.data.train_ds.tfms\n",
    "    rand_int = np.random.randint(len(img))\n",
    "    [img[rand_int].apply_tfms(tfms).show(ax=ax) for i, ax in enumerate(\n",
    "            plt.subplots(rows, cols, figsize=figsize)[1].flatten())]\n",
    "    plt.show()\n",
    "    return learn\n",
    "\n",
    "def show_multi_img_tfms(learn, rows=3, cols=3, figsize=(8, 8)):\n",
    "    xb, yb = learn.data.one_batch()\n",
    "    tfms = learn.data.train_ds.tfms\n",
    "    for i in range(len(xb)):\n",
    "        xb[i] = Image(xb[i]).apply_tfms(tfms).data\n",
    "    for cb in learn.callback_fns:\n",
    "        try:\n",
    "            cb_fn = partial(cb.func, **cb.keywords)\n",
    "            [Image(cb_fn(learn).on_batch_begin(\n",
    "                        xb, yb, True)['last_input'][0]).show(ax=ax)\n",
    "                for i, ax in enumerate(\n",
    "                    plt.subplots(rows, cols, figsize=figsize)[1].flatten())]\n",
    "            plt.show()\n",
    "            break\n",
    "        except:\n",
    "            plt.close('all')\n",
    "    return learn\n",
    "\n",
    "def show_tfms(learn, rows=3, cols=3, figsize=(8, 8)):\n",
    "    xb, yb = learn.data.one_batch()\n",
    "    rand_int = np.random.randint(len(xb))\n",
    "    rand_img = Image(xb[rand_int])\n",
    "    tfms = learn.data.train_ds.tfms\n",
    "    for i in range(len(xb)):\n",
    "        xb[i] = Image(xb[i]).apply_tfms(tfms).data\n",
    "    cb_tfms = 0\n",
    "    for cb in learn.callback_fns:\n",
    "        if hasattr(cb, 'keywords') and hasattr(get_fn(cb), 'on_batch_begin'):\n",
    "            cb_fn = partial(get_fn(cb), **cb.keywords)\n",
    "            try:\n",
    "                fig = plt.subplots(rows, cols, figsize=figsize)[1].flatten()\n",
    "                plt.suptitle(get_fn(cb).__name__, size=14)\n",
    "                [Image(cb_fn(learn).on_batch_begin(\n",
    "                            xb, yb, True)['last_input'][0]).show(ax=ax)\n",
    "                    for i, ax in enumerate(fig)]\n",
    "                plt.show()\n",
    "                cb_tfms += 1\n",
    "                break\n",
    "            except:\n",
    "                plt.close('all')\n",
    "    if cb_tfms == 0:\n",
    "        if tfms is not None:\n",
    "            t_ = []\n",
    "            for t in learn.data.train_ds.tfms: t_.append(get_fn(t).__name__)\n",
    "            title = f\"{str(t_)[1:-1]} transforms applied\"\n",
    "        else: title = f'No transform applied'\n",
    "        rand_int = np.random.randint(len(xb))\n",
    "        fig = plt.subplots(rows, cols, figsize=figsize)[1].flatten()\n",
    "        plt.suptitle(title, size=14)\n",
    "        [rand_img.apply_tfms(tfms).show(ax=ax) for i, ax in enumerate(fig)]\n",
    "        plt.show()\n",
    "    return learn\n",
    "\n",
    "Learner.show_tfms = show_tfms\n",
    "Learner.show_multi_img_tfms = show_multi_img_tfms\n",
    "Learner.show_single_img_tfms = show_single_img_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-15T14:36:54.838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "{\n",
       "    const ip = IPython.notebook\n",
       "    if (ip) {\n",
       "        ip.save_notebook()\n",
       "        console.log('a')\n",
       "        const s = `!python notebook2script.py ${ip.notebook_name}`\n",
       "        if (ip.kernel) { ip.kernel.execute(s) }\n",
       "    }\n",
       "    }"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "{\n",
       "    const ip = IPython.notebook\n",
       "    if (ip) {\n",
       "        ip.save_notebook()\n",
       "        console.log('a')\n",
       "        const s = `!python notebook2script.py ${ip.notebook_name}`\n",
       "        if (ip.kernel) { ip.kernel.execute(s) }\n",
       "    }\n",
       "    }"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try: from exp.nb_utils import *\n",
    "except ImportError: from .nb_utils import *\n",
    "nb_auto_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-v1",
   "language": "python",
   "name": "fastai-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
