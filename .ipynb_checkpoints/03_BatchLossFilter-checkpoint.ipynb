{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:49:50.146925Z",
     "start_time": "2019-10-15T19:49:50.135216Z"
    },
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "utils.load_extension('collapsible_headings/main')\n",
       "utils.load_extension('hide_input/main')\n",
       "utils.load_extension('autosavetime/main')\n",
       "utils.load_extension('execute_time/ExecuteTime')\n",
       "utils.load_extension('code_prettify/code_prettify')\n",
       "utils.load_extension('scroll_down/main')\n",
       "utils.load_extension('jupyter-js-widgets/extension')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "utils.load_extension('collapsible_headings/main')\n",
    "utils.load_extension('hide_input/main')\n",
    "utils.load_extension('autosavetime/main')\n",
    "utils.load_extension('execute_time/ExecuteTime')\n",
    "utils.load_extension('code_prettify/code_prettify')\n",
    "utils.load_extension('scroll_down/main')\n",
    "utils.load_extension('jupyter-js-widgets/extension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:30:35.758556Z",
     "start_time": "2019-10-16T12:30:35.753604Z"
    },
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignacio Oguiza - email: oguiza@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BatchLossFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I’d like to share with you a new callback I have created that has worked very well on some of my datasets.\n",
    "\n",
    "Last week @Redknight wrote a [post](https://forums.fast.ai/t/interesting-accelerating-deep-learning-by-focusing-on-the-biggest-losers/56091?u=oguiza) about this [paper](https://arxiv.org/pdf/1910.00762):\n",
    "\n",
    "Accelerating Deep Learning by Focusing on the Biggest Losers.\n",
    "\n",
    "The paper describes **Selective-Backprop**, a technique that accelerates the training of deep neural networks (DNNs) by **prioritizing examples with high loss at each iteration**. \n",
    "\n",
    "In parallel I also read a tweet by David Page:\n",
    "\n",
    "<img src=\"./images/tweet_blf.jpg\">\n",
    "\n",
    "The idea really resonated with me. I’ve always thought that it’d be good to spend most of the time learning about the most difficult examples. This seems a good way to do it, so I decided to try it. \n",
    "\n",
    "The paper’s code base in Pytorch is publically available [here](https://anonymous.4open.science/r/c6d4060d-bdac-4d31-839e-8579650255b3/). \n",
    "\n",
    "However, I thought I’d rather implement the idea with a different, simpler approach. The idea is this: identify those items within each batch that are responsible for a high % (I chose 90%) of the total batch loss, and remove the rest of the samples. In this way, your model will **dynamically focus on the high loss/ most difficult samples**. The percentage of samples remaining will vary per batch and along training as you’ll see.\n",
    "\n",
    "I’ve run a test in CIFAR10. Here these are the results:\n",
    "\n",
    "1) **Time to train** (100 epochs):  15.2 less time to train (in spite of the additional overhead)\n",
    "\n",
    "<img src=\"./images/time_blf.jpg\">\n",
    "\n",
    "2) **Accuracy**: same as the baseline model (at least in 100 epochs)\n",
    "\n",
    "<img src=\"./images/accuracy_blf.jpg\"> \n",
    "\n",
    "However, training is more smooth, and there’s a significant different in terms of validation loss. I believe that with a longer training there could be a difference in accuracy. But I have not confirmed this yet.\n",
    "\n",
    "3) **Validation loss**: much lower and smoother.\n",
    "\n",
    "<img src=\"./images/valid_loss_blf.jpg\">\n",
    "\n",
    "4) **Selected samples per batch**: This is very interesting in my opinion, as it shows the % of samples that make up 90% of the total batch loss. As you can see, 90% of the total loss is initially made by a large % of batch samples, but as training progresses, it dynamically focuses on the most difficult samples. This samples are not necessarily the same all the time, as they are chosen for each batch. In the end, the model will be focused on 12% of the most difficult samples. This is why training takes less time.\n",
    "\n",
    "Note: \n",
    "There are actually 2 hyperparameters: min_loss_perc: select samples that make a at least that %, and min_samples_perc: select at least a given % of highest losses. Both can be used at the same time. In my case I just used min_loss_perc.\n",
    "\n",
    "<img src=\"./images/sel_samples_blf.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:40:50.068877Z",
     "start_time": "2019-10-16T12:40:49.809883Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:40:51.571254Z",
     "start_time": "2019-10-16T12:40:50.439628Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai_extensions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:40:53.710929Z",
     "start_time": "2019-10-16T12:40:53.675256Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchLossFilterCallback(LearnerCallback):\n",
    "    _order = -20\n",
    "\n",
    "    def __init__(self, learn:Learner, min_sample_perc:float=0., min_loss_perc:float=0.):\n",
    "        super().__init__(learn)\n",
    "        assert min_sample_perc >0. or min_loss_perc > 0., 'min_sample_perc <= 0 and min_loss_perc <= 0'\n",
    "        self.min_sample_perc, self.min_loss_perc = min_sample_perc, min_loss_perc\n",
    "        self.learn = learn\n",
    "        self.model = learn.model\n",
    "        self.crit = learn.loss_func\n",
    "        if hasattr(self.crit, 'reduction'):  self.red = self.crit.reduction\n",
    "        self.sel_losses_sum, self.losses_sum = 0., 0.\n",
    "        self.sel_samples, self.samples = 0., 0.\n",
    "        self.recorder.add_metric_names([\"loss_perc\", \"samp_perc\"])\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        \"Set the inner value to 0.\"\n",
    "        self.sel_losses_sum, self.losses_sum = 0., 0.\n",
    "        self.sel_samples, self.samples = 0., 0.\n",
    "    \n",
    "    def on_batch_begin(self, last_input, last_target, train, epoch, **kwargs):\n",
    "        if not train or epoch == 0: return\n",
    "        if hasattr(self.crit, 'reduction'):  setattr(self.crit, 'reduction', 'none')\n",
    "        with torch.no_grad():  self.losses = np.array(self.crit(self.model(last_input), last_target))\n",
    "        if hasattr(self.crit, 'reduction'):  setattr(self.crit, 'reduction', self.red)\n",
    "        self.get_loss_idxs()\n",
    "        self.sel_losses_sum += self.losses[self.idxs].sum()\n",
    "        self.losses_sum += self.losses.sum()\n",
    "        self.sel_samples += len(self.idxs)\n",
    "        self.samples += len(self.losses)\n",
    "        return {\"last_input\": last_input[self.idxs], \"last_target\": last_target[self.idxs]}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):\n",
    "        loss_perc = self.sel_losses_sum / self.losses_sum if epoch > 0 else 1.\n",
    "        sample_perc = self.sel_samples / self.samples if epoch > 0 else 1.\n",
    "        return add_metrics(last_metrics, [loss_perc, sample_perc])\n",
    "    \n",
    "    def on_train_end(self, **kwargs):\n",
    "        \"\"\"At the end of training this calleback will be removed\"\"\"\n",
    "        if hasattr(self.learn.loss_func, 'reduction'):  setattr(self.learn.loss_func, 'reduction', self.red)\n",
    "        drop_cb_fn(self.learn, 'TopLossesCallback')\n",
    "        \n",
    "    def get_loss_idxs(self):\n",
    "        idxs = np.argsort(self.losses)[::-1]\n",
    "        sample_max = math.ceil(len(idxs) * self.min_sample_perc)\n",
    "        self.losses /= self.losses.sum()\n",
    "        loss_max = np.argmax(self.losses[idxs].cumsum() >= self.min_loss_perc) + 1\n",
    "        self.idxs =  list(idxs[:max(sample_max, loss_max)])\n",
    "        \n",
    "\n",
    "def batch_loss_filter(learn:Learner, min_sample_perc:float=0., min_loss_perc:float=.9)->Learner:\n",
    "    learn.callback_fns.append(partial(BatchLossFilterCallback, min_sample_perc=min_sample_perc, \n",
    "                                      min_loss_perc=min_loss_perc))\n",
    "    return learn\n",
    "\n",
    "Learner.batch_loss_filter = batch_loss_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:40:56.942901Z",
     "start_time": "2019-10-16T12:40:56.121053Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (50000 items)\n",
       "x: ImageList\n",
       "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
       "y: CategoryList\n",
       "ship,ship,ship,ship,ship\n",
       "Path: /home/oguizadl/.fastai/data/cifar10/train;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: ImageList\n",
       "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
       "y: CategoryList\n",
       "ship,ship,ship,ship,ship\n",
       "Path: /home/oguizadl/.fastai/data/cifar10/test;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 128\n",
    "path = untar_data(URLs.CIFAR)\n",
    "tfms = get_transforms()\n",
    "data = (ItemLists('.',\n",
    "                  ImageList.from_folder(path / 'train'),\n",
    "                  ImageList.from_folder(path / 'test'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms)\n",
    "        .databunch(bs=bs, val_bs=bs * 2)\n",
    "        .normalize(cifar_stats))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:41:05.662282Z",
     "start_time": "2019-10-16T12:41:00.048276Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = models.WideResNet(num_groups=3, N=4, num_classes=10, k=2, start_nf=32).to(device)\n",
    "xb,yb=next(iter(data.train_dl))\n",
    "with torch.no_grad():\n",
    "    losses = np.array(nn.CrossEntropyLoss(reduction='none')(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:41:05.690262Z",
     "start_time": "2019-10-16T12:41:05.664677Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_loss_idxs(losses, min_sample_perc=0., min_loss_perc=0.):\n",
    "    idxs = np.argsort(losses)[::-1]\n",
    "    sample_max = math.ceil(len(idxs) * min_sample_perc)\n",
    "    losses /= losses.sum()\n",
    "    loss_max = np.argmax(losses[idxs].cumsum() >= min_loss_perc) + 1\n",
    "    return list(idxs[:max(sample_max, loss_max)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:41:08.259870Z",
     "start_time": "2019-10-16T12:41:05.692415Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.2 µs ± 1.27 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit get_loss_idxs(losses, min_sample_perc=0., min_loss_perc=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:41:20.411510Z",
     "start_time": "2019-10-16T12:41:20.175763Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = models.WideResNet(num_groups=3, N=4, num_classes=10, k=2, start_nf=32).to(device)\n",
    "learn = Learner(data, model, metrics=accuracy).batch_loss_filter(min_loss_perc=.9)\n",
    "learn.save('stage-0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:47:40.339040Z",
     "start_time": "2019-10-16T08:41:07.088710Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss_perc</th>\n",
       "      <th>samp_perc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.453295</td>\n",
       "      <td>1.385631</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.523884</td>\n",
       "      <td>1.208875</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>0.902210</td>\n",
       "      <td>0.754207</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.420627</td>\n",
       "      <td>1.114552</td>\n",
       "      <td>0.607900</td>\n",
       "      <td>0.901955</td>\n",
       "      <td>0.731991</td>\n",
       "      <td>00:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.338659</td>\n",
       "      <td>0.988463</td>\n",
       "      <td>0.657500</td>\n",
       "      <td>0.902135</td>\n",
       "      <td>0.696995</td>\n",
       "      <td>00:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.314952</td>\n",
       "      <td>0.919686</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.901948</td>\n",
       "      <td>0.658974</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.234776</td>\n",
       "      <td>0.808278</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.901890</td>\n",
       "      <td>0.622937</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.199178</td>\n",
       "      <td>0.729833</td>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.901855</td>\n",
       "      <td>0.580669</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.150283</td>\n",
       "      <td>0.626935</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.901864</td>\n",
       "      <td>0.547095</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.092753</td>\n",
       "      <td>0.678280</td>\n",
       "      <td>0.768100</td>\n",
       "      <td>0.901998</td>\n",
       "      <td>0.518970</td>\n",
       "      <td>00:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.069250</td>\n",
       "      <td>0.584593</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.901962</td>\n",
       "      <td>0.485557</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.070438</td>\n",
       "      <td>0.524076</td>\n",
       "      <td>0.822700</td>\n",
       "      <td>0.902005</td>\n",
       "      <td>0.469772</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.030933</td>\n",
       "      <td>0.467221</td>\n",
       "      <td>0.844200</td>\n",
       "      <td>0.902079</td>\n",
       "      <td>0.455068</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.031671</td>\n",
       "      <td>0.495873</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.901991</td>\n",
       "      <td>0.437280</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.986188</td>\n",
       "      <td>0.479805</td>\n",
       "      <td>0.832400</td>\n",
       "      <td>0.902101</td>\n",
       "      <td>0.424339</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.960560</td>\n",
       "      <td>0.458648</td>\n",
       "      <td>0.845600</td>\n",
       "      <td>0.902090</td>\n",
       "      <td>0.407111</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.968091</td>\n",
       "      <td>0.417590</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.902270</td>\n",
       "      <td>0.395653</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.946837</td>\n",
       "      <td>0.447832</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>0.902171</td>\n",
       "      <td>0.384796</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.909196</td>\n",
       "      <td>0.367186</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>0.902151</td>\n",
       "      <td>0.377284</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.889021</td>\n",
       "      <td>0.386177</td>\n",
       "      <td>0.868500</td>\n",
       "      <td>0.902217</td>\n",
       "      <td>0.369071</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.849118</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>0.902269</td>\n",
       "      <td>0.361999</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.901207</td>\n",
       "      <td>0.370960</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.902360</td>\n",
       "      <td>0.346374</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.823922</td>\n",
       "      <td>0.352118</td>\n",
       "      <td>0.880600</td>\n",
       "      <td>0.902377</td>\n",
       "      <td>0.346134</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.798266</td>\n",
       "      <td>0.305606</td>\n",
       "      <td>0.896300</td>\n",
       "      <td>0.902443</td>\n",
       "      <td>0.337520</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.822596</td>\n",
       "      <td>0.313461</td>\n",
       "      <td>0.894600</td>\n",
       "      <td>0.902437</td>\n",
       "      <td>0.324880</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.794645</td>\n",
       "      <td>0.303263</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.902420</td>\n",
       "      <td>0.321615</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.768486</td>\n",
       "      <td>0.302518</td>\n",
       "      <td>0.898700</td>\n",
       "      <td>0.902419</td>\n",
       "      <td>0.316827</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.750348</td>\n",
       "      <td>0.295101</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>0.902583</td>\n",
       "      <td>0.307051</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.775783</td>\n",
       "      <td>0.301773</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.902466</td>\n",
       "      <td>0.300701</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.716362</td>\n",
       "      <td>0.276501</td>\n",
       "      <td>0.908400</td>\n",
       "      <td>0.902729</td>\n",
       "      <td>0.296995</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.643077</td>\n",
       "      <td>0.291353</td>\n",
       "      <td>0.901900</td>\n",
       "      <td>0.902625</td>\n",
       "      <td>0.287841</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.668315</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>0.902677</td>\n",
       "      <td>0.278185</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.649931</td>\n",
       "      <td>0.246358</td>\n",
       "      <td>0.916800</td>\n",
       "      <td>0.902705</td>\n",
       "      <td>0.275481</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.620367</td>\n",
       "      <td>0.267693</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>0.902734</td>\n",
       "      <td>0.269952</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.661274</td>\n",
       "      <td>0.252214</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>0.902718</td>\n",
       "      <td>0.261118</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.600443</td>\n",
       "      <td>0.286478</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.902838</td>\n",
       "      <td>0.255489</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.579304</td>\n",
       "      <td>0.246681</td>\n",
       "      <td>0.918100</td>\n",
       "      <td>0.903316</td>\n",
       "      <td>0.250921</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.585846</td>\n",
       "      <td>0.267625</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.903015</td>\n",
       "      <td>0.244030</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.551341</td>\n",
       "      <td>0.257240</td>\n",
       "      <td>0.916600</td>\n",
       "      <td>0.902941</td>\n",
       "      <td>0.241046</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.542043</td>\n",
       "      <td>0.240156</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>0.903072</td>\n",
       "      <td>0.233173</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.577572</td>\n",
       "      <td>0.239757</td>\n",
       "      <td>0.919300</td>\n",
       "      <td>0.903258</td>\n",
       "      <td>0.231631</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.537023</td>\n",
       "      <td>0.263071</td>\n",
       "      <td>0.911700</td>\n",
       "      <td>0.903118</td>\n",
       "      <td>0.232953</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.509553</td>\n",
       "      <td>0.214428</td>\n",
       "      <td>0.929100</td>\n",
       "      <td>0.903101</td>\n",
       "      <td>0.222456</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.549651</td>\n",
       "      <td>0.229536</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.903237</td>\n",
       "      <td>0.218069</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.510820</td>\n",
       "      <td>0.243997</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>0.903231</td>\n",
       "      <td>0.221474</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.506490</td>\n",
       "      <td>0.219203</td>\n",
       "      <td>0.928400</td>\n",
       "      <td>0.903119</td>\n",
       "      <td>0.215605</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.473091</td>\n",
       "      <td>0.225021</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>0.903105</td>\n",
       "      <td>0.216486</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.535196</td>\n",
       "      <td>0.264205</td>\n",
       "      <td>0.915100</td>\n",
       "      <td>0.903239</td>\n",
       "      <td>0.209876</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.429336</td>\n",
       "      <td>0.212874</td>\n",
       "      <td>0.929500</td>\n",
       "      <td>0.903140</td>\n",
       "      <td>0.208574</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.230872</td>\n",
       "      <td>0.927400</td>\n",
       "      <td>0.903546</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.438266</td>\n",
       "      <td>0.220423</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>0.903425</td>\n",
       "      <td>0.195553</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.435072</td>\n",
       "      <td>0.221873</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>0.903618</td>\n",
       "      <td>0.196635</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.428855</td>\n",
       "      <td>0.221521</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.903386</td>\n",
       "      <td>0.192708</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.428426</td>\n",
       "      <td>0.227326</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.184675</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.425181</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.903484</td>\n",
       "      <td>0.183854</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.406586</td>\n",
       "      <td>0.225725</td>\n",
       "      <td>0.928700</td>\n",
       "      <td>0.903629</td>\n",
       "      <td>0.179988</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.430910</td>\n",
       "      <td>0.216181</td>\n",
       "      <td>0.930700</td>\n",
       "      <td>0.903766</td>\n",
       "      <td>0.175681</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.422010</td>\n",
       "      <td>0.214040</td>\n",
       "      <td>0.932800</td>\n",
       "      <td>0.903685</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.451596</td>\n",
       "      <td>0.217220</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.175361</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.403622</td>\n",
       "      <td>0.213258</td>\n",
       "      <td>0.934900</td>\n",
       "      <td>0.903745</td>\n",
       "      <td>0.172997</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.363377</td>\n",
       "      <td>0.217208</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.904239</td>\n",
       "      <td>0.168349</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.326432</td>\n",
       "      <td>0.207387</td>\n",
       "      <td>0.934400</td>\n",
       "      <td>0.903678</td>\n",
       "      <td>0.167187</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.343536</td>\n",
       "      <td>0.222265</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.904241</td>\n",
       "      <td>0.162039</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.349382</td>\n",
       "      <td>0.202414</td>\n",
       "      <td>0.938200</td>\n",
       "      <td>0.904408</td>\n",
       "      <td>0.163141</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.322816</td>\n",
       "      <td>0.203811</td>\n",
       "      <td>0.936100</td>\n",
       "      <td>0.904387</td>\n",
       "      <td>0.159615</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.332866</td>\n",
       "      <td>0.206022</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.904445</td>\n",
       "      <td>0.160397</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.301094</td>\n",
       "      <td>0.211222</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>0.904243</td>\n",
       "      <td>0.161078</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.261654</td>\n",
       "      <td>0.211531</td>\n",
       "      <td>0.936500</td>\n",
       "      <td>0.904564</td>\n",
       "      <td>0.157933</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.329442</td>\n",
       "      <td>0.211100</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>0.905184</td>\n",
       "      <td>0.149639</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.296931</td>\n",
       "      <td>0.206738</td>\n",
       "      <td>0.938200</td>\n",
       "      <td>0.905743</td>\n",
       "      <td>0.148317</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.290859</td>\n",
       "      <td>0.205369</td>\n",
       "      <td>0.940400</td>\n",
       "      <td>0.905814</td>\n",
       "      <td>0.147456</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.275567</td>\n",
       "      <td>0.211211</td>\n",
       "      <td>0.937600</td>\n",
       "      <td>0.906330</td>\n",
       "      <td>0.143830</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.243659</td>\n",
       "      <td>0.207840</td>\n",
       "      <td>0.940100</td>\n",
       "      <td>0.906861</td>\n",
       "      <td>0.143590</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.205215</td>\n",
       "      <td>0.205796</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.907119</td>\n",
       "      <td>0.143069</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.208034</td>\n",
       "      <td>0.201922</td>\n",
       "      <td>0.942200</td>\n",
       "      <td>0.908119</td>\n",
       "      <td>0.135557</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.238317</td>\n",
       "      <td>0.211227</td>\n",
       "      <td>0.940400</td>\n",
       "      <td>0.909814</td>\n",
       "      <td>0.131851</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.210464</td>\n",
       "      <td>0.216041</td>\n",
       "      <td>0.939500</td>\n",
       "      <td>0.910118</td>\n",
       "      <td>0.131611</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.228930</td>\n",
       "      <td>0.197036</td>\n",
       "      <td>0.943300</td>\n",
       "      <td>0.911599</td>\n",
       "      <td>0.129768</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.156541</td>\n",
       "      <td>0.197168</td>\n",
       "      <td>0.945600</td>\n",
       "      <td>0.910187</td>\n",
       "      <td>0.134235</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.157514</td>\n",
       "      <td>0.202404</td>\n",
       "      <td>0.942200</td>\n",
       "      <td>0.912274</td>\n",
       "      <td>0.129768</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.165035</td>\n",
       "      <td>0.206366</td>\n",
       "      <td>0.944200</td>\n",
       "      <td>0.911295</td>\n",
       "      <td>0.128646</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.120750</td>\n",
       "      <td>0.202003</td>\n",
       "      <td>0.944600</td>\n",
       "      <td>0.913740</td>\n",
       "      <td>0.125240</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.146101</td>\n",
       "      <td>0.214029</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.915474</td>\n",
       "      <td>0.126763</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.130147</td>\n",
       "      <td>0.194949</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>0.917262</td>\n",
       "      <td>0.122877</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.097316</td>\n",
       "      <td>0.196396</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>0.915142</td>\n",
       "      <td>0.126282</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.118518</td>\n",
       "      <td>0.196611</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>0.918104</td>\n",
       "      <td>0.120353</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.117263</td>\n",
       "      <td>0.193720</td>\n",
       "      <td>0.946600</td>\n",
       "      <td>0.918592</td>\n",
       "      <td>0.121474</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.115145</td>\n",
       "      <td>0.194640</td>\n",
       "      <td>0.947700</td>\n",
       "      <td>0.917407</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.096233</td>\n",
       "      <td>0.196569</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.916761</td>\n",
       "      <td>0.123057</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.084011</td>\n",
       "      <td>0.195848</td>\n",
       "      <td>0.949900</td>\n",
       "      <td>0.918475</td>\n",
       "      <td>0.122115</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.073634</td>\n",
       "      <td>0.196037</td>\n",
       "      <td>0.948700</td>\n",
       "      <td>0.916842</td>\n",
       "      <td>0.124099</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.092587</td>\n",
       "      <td>0.201659</td>\n",
       "      <td>0.949100</td>\n",
       "      <td>0.920499</td>\n",
       "      <td>0.121414</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.080026</td>\n",
       "      <td>0.194939</td>\n",
       "      <td>0.949400</td>\n",
       "      <td>0.917658</td>\n",
       "      <td>0.122676</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.069848</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.949500</td>\n",
       "      <td>0.919620</td>\n",
       "      <td>0.120032</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.072962</td>\n",
       "      <td>0.198985</td>\n",
       "      <td>0.949300</td>\n",
       "      <td>0.917574</td>\n",
       "      <td>0.123177</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.060722</td>\n",
       "      <td>0.192767</td>\n",
       "      <td>0.950400</td>\n",
       "      <td>0.921548</td>\n",
       "      <td>0.120633</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.066263</td>\n",
       "      <td>0.195414</td>\n",
       "      <td>0.950200</td>\n",
       "      <td>0.918124</td>\n",
       "      <td>0.124479</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.063223</td>\n",
       "      <td>0.194221</td>\n",
       "      <td>0.950600</td>\n",
       "      <td>0.919665</td>\n",
       "      <td>0.120773</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.066589</td>\n",
       "      <td>0.190727</td>\n",
       "      <td>0.950200</td>\n",
       "      <td>0.917662</td>\n",
       "      <td>0.123618</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.076418</td>\n",
       "      <td>0.193097</td>\n",
       "      <td>0.950800</td>\n",
       "      <td>0.919964</td>\n",
       "      <td>0.121534</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.066630</td>\n",
       "      <td>0.196830</td>\n",
       "      <td>0.950200</td>\n",
       "      <td>0.918654</td>\n",
       "      <td>0.122857</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.load('stage-0')\n",
    "learn.fit_one_cycle(100)\n",
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You'll only need to clone the repo. I have added BatchLossFilter as another fastai_extensions. so all you need to do is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:37:44.189555Z",
     "start_time": "2019-10-16T12:37:44.167384Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai_extensions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then prepare your data as you would normally do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:37:47.108907Z",
     "start_time": "2019-10-16T12:37:46.236297Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (50000 items)\n",
       "x: ImageList\n",
       "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
       "y: CategoryList\n",
       "ship,ship,ship,ship,ship\n",
       "Path: /home/oguizadl/.fastai/data/cifar10/train;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: ImageList\n",
       "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
       "y: CategoryList\n",
       "ship,ship,ship,ship,ship\n",
       "Path: /home/oguizadl/.fastai/data/cifar10/test;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 128\n",
    "path = untar_data(URLs.CIFAR)\n",
    "tfms = get_transforms()\n",
    "data = (ItemLists('.',\n",
    "                  ImageList.from_folder(path / 'train'),\n",
    "                  ImageList.from_folder(path / 'test'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms)\n",
    "        .databunch(bs=bs, val_bs=bs * 2)\n",
    "        .normalize(cifar_stats))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Build your learner, and add batch_loss_filter. You can modify the min_loss_perc and or/ min_sample_perc hyperparameters if you wish, or leave them with their default values: min_loss_perc = .9, min_sample_perc=.0, which will select the top items responsible for 90% of the loss per batch, independently of how many they are (you may add a contraint if you prefer adding min_sample_perc equal to .1, .2, etc), but my current view is that this doesn't bring any additional value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:37:48.321230Z",
     "start_time": "2019-10-16T12:37:48.179842Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = models.WideResNet(num_groups=3, N=4, num_classes=10, k=2, start_nf=32).to(device)\n",
    "learn = Learner(data, model, metrics=accuracy).batch_loss_filter(min_loss_perc=.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And now you are ready to train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Good luck with your experiments!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-v1",
   "language": "python",
   "name": "fastai-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
